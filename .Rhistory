library(segsimflex)
install.packages("path/to/newPackage_0.1.0.tar.gz",
repos=NULL, type="source")
usethis::use_mit_license()
devtools::document()
devtools::document()
install.packages('keras')
library(tensorflow)
tf$constant('Hello world')
Y
tf$constant('Hello world')
library(keras)
library(keras)
install_keras()
library(keras)
install_keras()
install.packages('keras')
install.packages("keras")
library(keras)
library(tensorflow)
tf$constant('Hello world')
install_keras()
library(keras)
install_keras()
remove.packages("keras")
install.packages('keras')
install.packages('keras')
library(keras)
install_keras()
n
library(tensorflow)
tf$constant('Hello world')
# packages
library(tidyverse)
library(tidymodels)
library(tidytext)
library(keras)
library(tensorflow)
# data location
url <- 'https://raw.githubusercontent.com/pstat197/pstat197a/main/materials/labs/lab6-nn/data/claims-clean.csv'
# read in data
clean <- read_csv(url)
# partition
set.seed(102722)
partitions <- clean %>%
mutate(text_clean = str_trim(text_clean)) %>%
filter(str_length(text_clean) > 5) %>%
initial_split(prop = 0.8)
# partition
set.seed(102722)
partitions <- clean %>%
mutate(text_clean = str_trim(text_clean)) %>%
filter(str_length(text_clean) > 5) %>%
initial_split(prop = 0.8)
train_dtm <- training(partitions) %>%
unnest_tokens(output = 'token',
input = text_clean) %>%
group_by(.id, bclass) %>%
count(token) %>%
bind_tf_idf(term = token,
document = .id,
n = n) %>%
pivot_wider(id_cols = c(.id, bclass),
names_from = token,
values_from = tf_idf,
values_fill = 0) %>%
ungroup()
# extract first ten features
x_train <- train_dtm %>%
ungroup() %>%
select(-.id, -bclass) %>%
select(1:10) %>%
as.matrix()
# extract labels and coerce to binary
y_train <- train_dtm %>%
pull(bclass) %>%
factor() %>%
as.numeric() - 1
# specify model type
model <- keras_model_sequential(input_shape = 10)
summary(model)
# add output layer
model <- model %>% layer_dense(1)
summary(model)
model <- model %>%
layer_activation(activation = 'sigmoid')
model %>% compile(
loss = 'binary_crossentropy',
optimizer = optimizer_sgd(),
metrics = 'binary_accuracy'
)
history <- model %>%
fit(x = x_train,
y = y_train,
epochs = 10)
# retrieve weights
get_weights(model)
history <- model %>%
fit(x = x_train,
y = y_train,
epochs = 10)
# retrieve weights
get_weights(model)
# evaluate on specified data
evaluate(model, x_train, y_train)
# compute predictions
model(x_train) %>% head()
# store full DTM as a matrix
x_train <- train_dtm %>%
select(-bclass, -.id) %>%
as.matrix()
summary(model)
model %>%
compile(
loss = 'binary_crossentropy',
optimizer = optimizer_sgd(),
metrics = 'binary_accuracy'
)
plot(history)
# change the optimizer
model %>%
compile(
loss = 'binary_crossentropy',
optimizer = 'adam',
metrics = 'binary_accuracy'
)
# re-train
history <- model %>%
fit(x = x_train,
y = y_train,
epochs = 10)
# evaluate on specified data
evaluate(model, x_train, y_train)
# add output layer
model <- model %>% layer_dense(1)
# specify model type
model <- keras_model_sequential(input_shape = 10)
summary(model)
# add output layer
model <- model %>% layer_dense(1)
summary(model)
model <- model %>%
layer_activation(activation = 'sigmoid')
model %>% compile(
loss = 'binary_crossentropy',
optimizer = optimizer_sgd(),
metrics = 'binary_accuracy'
)
history <- model %>%
fit(x = x_train,
y = y_train,
epochs = 10)
# retrieve weights
get_weights(model)
# packages
library(tidyverse)
library(tidymodels)
library(tidytext)
library(keras)
library(tensorflow)
# data location
url <- 'https://raw.githubusercontent.com/pstat197/pstat197a/main/materials/labs/lab6-nn/data/claims-clean.csv'
# read in data
clean <- read_csv(url)
# partition
set.seed(102722)
partitions <- clean %>%
mutate(text_clean = str_trim(text_clean)) %>%
filter(str_length(text_clean) > 5) %>%
initial_split(prop = 0.8)
train_dtm <- training(partitions) %>%
unnest_tokens(output = 'token',
input = text_clean) %>%
group_by(.id, bclass) %>%
count(token) %>%
bind_tf_idf(term = token,
document = .id,
n = n) %>%
pivot_wider(id_cols = c(.id, bclass),
names_from = token,
values_from = tf_idf,
values_fill = 0) %>%
ungroup()
# extract first ten features
x_train <- train_dtm %>%
ungroup() %>%
select(-.id, -bclass) %>%
select(1:10) %>%
as.matrix()
# extract labels and coerce to binary
y_train <- train_dtm %>%
pull(bclass) %>%
factor() %>%
as.numeric() - 1
# specify model type
model <- keras_model_sequential(input_shape = 10)
summary(model)
# add output layer
model <- model %>% layer_dense(1)
summary(model)
model <- model %>%
layer_activation(activation = 'sigmoid')
model %>% compile(
loss = 'binary_crossentropy',
optimizer = optimizer_sgd(),
metrics = 'binary_accuracy'
)
history <- model %>%
fit(x = x_train,
y = y_train,
epochs = 10)
# retrieve weights
get_weights(model)
# evaluate on specified data
evaluate(model, x_train, y_train)
# compute predictions
model(x_train) %>% head()
# store full DTM as a matrix
x_train <- train_dtm %>%
select(-bclass, -.id) %>%
as.matrix()
model <- keras_model_sequential(input_shape = ncol(x_train)) %>%
layer_dense(10) %>%
layer_dense(1) %>%
layer_activation(activation = 'sigmoid')
summary(model)
model %>%
compile(
loss = 'binary_crossentropy',
optimizer = optimizer_sgd(),
metrics = 'binary_accuracy'
)
history <- model %>%
fit(x = x_train,
y = y_train,
epochs = 50)
plot(history)
# change the optimizer
model %>%
compile(
loss = 'binary_crossentropy',
optimizer = 'adam',
metrics = 'binary_accuracy'
)
# re-train
history <- model %>%
fit(x = x_train,
y = y_train,
epochs = 10)
plot(history)
# redefine model
model <- keras_model_sequential(input_shape = ncol(x_train)) %>%
layer_dense(10) %>%
layer_dense(1) %>%
layer_activation(activation = 'sigmoid')
model %>%
compile(
loss = 'binary_crossentropy',
optimizer = 'adam',
metrics = 'binary_accuracy'
)
# train with validation split
history <- model %>%
fit(x = x_train,
y = y_train,
epochs = 20,
validation_split = 0.2)
plot(history)
devtools::build()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
